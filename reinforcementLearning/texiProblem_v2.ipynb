{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Taxi environment\n",
    "env = gym.make('Taxi-v3', render_mode='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-table: [[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [ -2.85394489  -2.31752643  -2.86464051  -2.31564717  -1.6445568\n",
      "  -11.31566284]\n",
      " [  0.24288      1.55357757   0.24277779   1.54836485   3.192\n",
      "   -7.44640005]\n",
      " ...\n",
      " [ -2.74682857   1.50973178  -2.842129    -2.6860032  -11.72839608\n",
      "  -11.31762096]\n",
      " [ -3.82845378  -3.78799993  -3.60279663  -0.805696   -11.3558004\n",
      "  -12.48238226]\n",
      " [  9.80099988   2.14426471  10.76282784  15.           0.\n",
      "    1.98876689]]\n",
      "Training completed after 2000 episodes\n"
     ]
    }
   ],
   "source": [
    "# initialize q-table\n",
    "state_size \t\t\t= env.observation_space.n\n",
    "action_size \t\t= env.action_space.n\n",
    "qtable \t\t\t\t= np.zeros((state_size, action_size))\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate \t\t= 0.9\n",
    "discount_rate \t\t= 0.8\n",
    "epsilon \t\t\t= 1.0\n",
    "decay_rate\t\t\t= 0.005\n",
    "\n",
    "# training variables\n",
    "num_episodes \t\t= 2000\n",
    "max_steps \t\t\t= 99 # per episode\n",
    "\n",
    "print(\"Training the agent...\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    # Reset the environment\n",
    "    state \t\t    = env.reset()\n",
    "    state \t\t\t= state[0]\n",
    "    step \t\t\t= 0\n",
    "    done \t\t\t= False\n",
    "\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        # Exploration-exploitation tradeoff\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            # Explore\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Exploit\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Take an action and observe the reward\n",
    "        #new_state, reward, done, info = env.step(action)\n",
    "        output \t\t= env.step(action)\n",
    "        new_state \t= output[0]\n",
    "        reward \t\t= output[1]\n",
    "        done \t\t= output[2]\n",
    "        info \t\t= output[3]\n",
    "\n",
    "        # Q-learning algorithm\n",
    "        qtable[state,action] = qtable[state,action] + learning_rate * (reward + discount_rate * np.max(qtable[new_state,:])-qtable[state,action])\n",
    "\n",
    "        # Update to our new state\n",
    "        state \t\t= new_state\n",
    "\n",
    "        # if done, finish episode\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    # Decrease epsilon\n",
    "    epsilon = 1.0 / (1.0 + decay_rate * episode)\n",
    "\n",
    "print(f\"Trained Q-table: {qtable}\")\n",
    "print(f\"Training completed after {num_episodes} episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "Episode finished after 14 timesteps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def visualize_agent(env, qtable, episodes=5, max_steps=100):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        print(f\"Episode {episode + 1}\\n\")\n",
    "        sleep(1)\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            clear_output(wait=True)\n",
    "            print(env.render())\n",
    "            sleep(0.5)  # Adjust the speed of the animation\n",
    "\n",
    "            # Choose action based on Q-table\n",
    "            action = np.argmax(qtable[state, :])\n",
    "            output = env.step(action)\n",
    "            new_state \t= output[0]\n",
    "            reward \t\t= output[1]\n",
    "            done \t\t= output[2]\n",
    "            info \t\t= output[3]            \n",
    "            \n",
    "            state = new_state\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode finished after {step + 1} timesteps\\n\")\n",
    "                sleep(2)\n",
    "                clear_output(wait=True)\n",
    "                break\n",
    "\n",
    "# Visualize the trained agent\n",
    "visualize_agent(env, qtable, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
