# -*- coding: utf-8 -*-
"""weightsRestructuringSVD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LOks0pwoePHWvDQKcYwFfBJ8R8HKXEml
"""

import numpy as np

from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.initializers import Constant
from tensorflow.keras.initializers import Initializer
from tensorflow.keras.layers import Dense, Flatten

# Load MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Preprocess data
X_train = X_train.reshape(-1, 784).astype('float32') / 255
X_test = X_test.reshape(-1, 784).astype('float32') / 255
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Custom initializers
class CustomKernelInitializer(Initializer):
    def __init__(self, weights):
        self.weights = weights

    def __call__(self, shape, dtype=None):
        return self.weights

class CustomBiasInitializer(Initializer):
    def __init__(self, biases):
        self.biases = biases

    def __call__(self, shape, dtype=None):
        return self.biases

energy_threshold = 0.9

# Function to count the node connections in the model
def count_node_connections(model):
    total_connections = 0
    for layer in model.layers:
        if isinstance(layer, Dense):
            weights = layer.get_weights()
            connections = 0
            for weight in weights: # Iterate over weights and biases
                connections += weight.size  # Calculate connections for each array
            print(f'Layer: {layer.name}, Connections: {connections}')
            total_connections += connections
    return total_connections

# Define the original model
original_model = Sequential([
    Dense(128, input_shape=(784,), activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

original_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the original model
original_model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the original model
loss, accuracy = original_model.evaluate(X_test, y_test)
print(f'Original Model Accuracy: {accuracy * 100:.2f}%')

original_model.summary()

# Count the node connections in the model
total_connections = count_node_connections(original_model)
print(f'Total node connections in the model: {total_connections}')

def apply_svd(weights, energy_threshold=0.2):
    # Step 1: Compute the SVD of the matrix
    U, s, Vt = np.linalg.svd(weights, full_matrices=False)

    # Step 2: Calculate the total energy (sum of squares of all singular values)
    total_energy = np.sum(s**2)

    # Step 3: Determine the threshold for retaining singular values (20% of the total energy)
    energy_target = energy_threshold * total_energy

    # Step 4: Find the number of singular values that together account for this 20% energy
    cumulative_energy = 0
    rank = 0
    for singular_value in s:
        cumulative_energy += singular_value**2
        rank += 1
        if cumulative_energy >= energy_target:
            break

    U_reduced = U[:, :rank]
    s_reduced = np.diag(s[:rank])
    Vt_reduced = Vt[:rank, :]
    new_weights_1 = np.dot(U_reduced, s_reduced)
    new_weights_2 = Vt_reduced

    return new_weights_1, new_weights_2, rank

def prune_weights(model, energy_threshold=0.2):

    #layers_to_prune = model.layers[:-1]
    layers_to_prune = model.layers

    # Create a new model with pruned layers
    new_model = Sequential()
    new_model.add(Flatten(input_shape=(784,)))

    for i, layer in enumerate(layers_to_prune):
        if isinstance(layer, Dense):
            weights, biases = layer.get_weights()

            print(i)

            new_weights_1, new_weights_2, rank = apply_svd(weights, energy_threshold)

            if np.isnan(new_weights_1).any():
              print(f"NaNs found in layer {layer.name}, weight matrix {i}")
            if np.isnan(new_weights_2).any():
              print(f"NaNs found in layer {layer.name}, weight matrix {i}")

            kernel_initializer = CustomKernelInitializer(new_weights_1)
            new_model.add(Dense(new_weights_1.shape[1], kernel_initializer = kernel_initializer, use_bias = False, activation = 'relu'))
            kernel_initializer = CustomKernelInitializer(new_weights_2)
            bias_initializer   = CustomBiasInitializer(biases)
            if i < 2:
              new_model.add(Dense(new_weights_2.shape[1], kernel_initializer = kernel_initializer, bias_initializer = bias_initializer, activation = 'relu'))
            else:
              new_model.add(Dense(new_weights_2.shape[1], kernel_initializer = kernel_initializer, bias_initializer = bias_initializer, activation = 'softmax'))

    return new_model

restructured_model = prune_weights(original_model, energy_threshold)

restructured_model.summary()

# Count the node connections in the model
total_connections = count_node_connections(restructured_model)
print(f'Total node connections in the model: {total_connections}')

# Compile the restructured model
restructured_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Retrain the restructured model
restructured_model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the restructured model
loss, accuracy = restructured_model.evaluate(X_test, y_test)
print(f'Restructured Model Accuracy: {accuracy * 100:.2f}%')
