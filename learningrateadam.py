# -*- coding: utf-8 -*-
"""learningRateAdam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zvdshC--9G75OYy4m__lJouhVPpyMbll
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize the data
x_train, x_test = x_train / 255.0, x_test / 255.0

# Build a simple neural network model
def build_model():
    model = Sequential([
        Flatten(input_shape=(28, 28)),
        Dense(128, activation='relu'),
        Dense(10, activation='softmax')
    ])
    return model

# Function to find the optimal learning rate range
def lr_finder(model, x_train, y_train, min_lr=1e-6, max_lr=1e-1, steps=100):
    lrs = np.logspace(np.log10(min_lr), np.log10(max_lr), num=steps)
    losses = []

    for lr in lrs:
        # Compile the model with a new learning rate
        model.compile(optimizer=Adam(learning_rate=lr),
                      loss='sparse_categorical_crossentropy')

        # Train for just one batch
        history = model.fit(x_train, y_train, batch_size=64, epochs=1, verbose=0)

        # Record the loss
        loss = history.history['loss'][0]
        losses.append(loss)

    return lrs, losses

# Run the learning rate finder
model = build_model()
lrs, losses = lr_finder(model, x_train, y_train)

# Plot the results to visualize the optimal learning rate range
plt.plot(lrs, losses)
plt.xscale('log')
plt.xlabel('Learning Rate')
plt.ylabel('Loss')
plt.title('Learning Rate Finder')
plt.show()

# Select optimal learning rate range based on the plot
# You should choose the learning rate where the loss starts decreasing rapidly,
# usually just before the loss starts increasing again.
optimal_lr = lrs[np.argmin(losses)]

# Learning Rate Scheduler: Exponential Decay
initial_lr = optimal_lr  # Use the best learning rate found
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=initial_lr,
    decay_steps=10000,
    decay_rate=0.96,
    staircase=True)

# Rebuild and compile the model using the learning rate scheduler
model = build_model()
model.compile(optimizer=Adam(learning_rate=lr_schedule),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model using the learning rate scheduler
history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f'Test accuracy: {test_acc:.4f}')